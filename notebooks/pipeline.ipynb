{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.padding import PKCS7\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import pyspark\n",
    "from nibabel.filebasedimages import FileBasedImage\n",
    "import cv2\n",
    "import pandas as pd\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_SCANS = \"../rawdata/ct_scans\"\n",
    "INFECTION_MASKS = \"../rawdata/infection_mask\"\n",
    "LUNGANDINFECTION_MASKS = \"../rawdata/lung_and_infection_mask\"\n",
    "LUNG_MASKS = \"../rawdata/lung_mask\"\n",
    "OUTPUT_DIR = \"../rawdata/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-998., -998., -999., ..., -998., -998., -998.],\n",
       "        [-998., -998., -999., ..., -998., -998., -998.],\n",
       "        [-998., -998., -999., ..., -998., -998., -998.],\n",
       "        ...,\n",
       "        [-998., -999., -998., ..., -998., -998., -998.],\n",
       "        [-998., -999., -998., ..., -998., -998., -998.],\n",
       "        [-998., -999., -998., ..., -998., -998., -998.]]),\n",
       " np.float64(-1021.0),\n",
       " np.float64(2996.0),\n",
       " (512, 512, 301))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_nii: FileBasedImage = nib.load(f\"{CT_SCANS}/coronacases_org_001.nii\")\n",
    "header = ct_nii.header\n",
    "affine = ct_nii.affine\n",
    "data = ct_nii.get_fdata()\n",
    "data = np.array(data)\n",
    "data[0], np.min(data), np.max(data), data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 19:34:10 WARN Utils: Your hostname, SerVer resolves to a loopback address: 127.0.1.1; using 192.168.1.250 instead (on interface enp3s0f0)\n",
      "24/12/07 19:34:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/07 19:34:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NIfTI_Processing\") \\\n",
    "    .config(\"spark.executor.memory\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"4\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Extract #####\n",
    "def extract_metadata():\n",
    "    # Extract metadata from the downloaded dataset\n",
    "    path = os.path.join(OUTPUT_DIR, \"metadata.csv\")\n",
    "    metadata = pd.read_csv(path)\n",
    "    logger.info(\"Metadata extracted successfully!\")\n",
    "    metadata.replace(\"../input/covid19-ct-scans/\", \"\", regex=True, inplace=True)\n",
    "    return metadata\n",
    "\n",
    "def extract_image_data(metadata: pd.DataFrame):\n",
    "    # Convert DataFrame to np array\n",
    "    image_paths: np.ndarray = metadata.to_numpy().flatten()\n",
    "    # Extract image data\n",
    "    def load_image(x):\n",
    "        path = os.path.join(OUTPUT_DIR, x)\n",
    "        nifti_data = nib.load(path)\n",
    "        header = nifti_data.header\n",
    "        affine = nifti_data.affine\n",
    "        image_data = nifti_data.get_fdata()\n",
    "        image_data = np.array(image_data)\n",
    "\n",
    "        return image_data, header, affine, x\n",
    "\n",
    "    image_rdds = sc.parallelize(image_paths).map(load_image)\n",
    "\n",
    "    return image_rdds\n",
    "\n",
    "def save_image_data(image_rdds: pyspark.rdd.PipelinedRDD):\n",
    "    # Save image data to disk\n",
    "    def save_image(x):\n",
    "        image_data, header, affine, file_name = x\n",
    "        nifti_data = nib.Nifti1Image(image_data, affine, header)\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        folder = file_name.split(\"/\")[0]\n",
    "        os.makedirs(f\"output/{folder}\", exist_ok=True)\n",
    "        nifti_data.to_filename(f\"output/{file_name}\")\n",
    "\n",
    "    image_rdds.foreach(save_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Transform #####\n",
    "\n",
    "# Resize the image's x and y dimensions to 256x256\n",
    "def resize_image(image_rdds: pyspark.rdd.PipelinedRDD):\n",
    "    def resize_image(x):\n",
    "        image_data, header, affine, file_name = x\n",
    "        resized_image = cv2.resize(image_data, (256, 256))\n",
    "        return resized_image, header, affine, file_name\n",
    "    \n",
    "    resized_image_rdds = image_rdds.map(resize_image)\n",
    "    logger.info(\"Image resized successfully!\")\n",
    "    return resized_image_rdds\n",
    "\n",
    "# Normalize the image data\n",
    "def normalize_image(image_rdds: pyspark.rdd.PipelinedRDD):\n",
    "    def normalize_image(x):\n",
    "        image_data, header, affine, file_name = x\n",
    "        normalized_image = cv2.normalize(image_data, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        return normalized_image, header, affine, file_name\n",
    "\n",
    "    normalized_image_rdds = image_rdds.map(normalize_image)\n",
    "    logger.info(\"Image normalized successfully!\")\n",
    "    return normalized_image_rdds\n",
    "\n",
    "# Noice reduction\n",
    "def denoise_image(image_rdds: pyspark.rdd.PipelinedRDD):\n",
    "    def denoise_image(x):\n",
    "        image_data, header, affine, file_name = x\n",
    "        denoised_image = cv2.fastNlMeansDenoising(image_data, None, 10, 7, 21)\n",
    "        return denoised_image, header, affine, file_name\n",
    "\n",
    "    denoised_image_rdds = image_rdds.map(denoise_image)\n",
    "    logger.info(\"Image denoised successfully!\")\n",
    "    return denoised_image_rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load #####\n",
    "# Save to disk\n",
    "def save_data(image_rdds: pyspark.rdd.PipelinedRDD):\n",
    "    def save_image(x):\n",
    "        image_data, header, affine, file_name = x\n",
    "        print(f\"Saving image: {file_name}\")\n",
    "        # Save image data: np array to disk\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        folder = file_name.split(\"/\")[0]\n",
    "        os.makedirs(f\"output/{folder}\", exist_ok=True)\n",
    "        image_data = np.array(image_data)\n",
    "        np.save(f\"output/{file_name}\", image_data)\n",
    "\n",
    "    image_rdds.foreach(save_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Metadata extracted successfully!\n",
      "24/12/07 20:24:09 ERROR Executor: Exception in task 66.0 in stage 11.0 (TID 858)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/07 20:24:09 WARN TaskSetManager: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/12/07 20:24:09 ERROR TaskSetManager: Task 66 in stage 11.0 failed 1 times; aborting job\n",
      "24/12/07 20:24:09 WARN TaskSetManager: Lost task 65.0 in stage 11.0 (TID 857) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:09 WARN TaskSetManager: Lost task 59.0 in stage 11.0 (TID 851) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:10 WARN TaskSetManager: Lost task 60.0 in stage 11.0 (TID 852) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:10 WARN TaskSetManager: Lost task 55.0 in stage 11.0 (TID 847) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:10 WARN TaskSetManager: Lost task 57.0 in stage 11.0 (TID 849) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:10 WARN TaskSetManager: Lost task 68.0 in stage 11.0 (TID 860) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:10 WARN TaskSetManager: Lost task 61.0 in stage 11.0 (TID 853) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "[Stage 11:>                                                       (0 + 64) / 72]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n    for x in iterator:\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\ncv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n    for x in iterator:\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\ncv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m rdds \u001b[38;5;241m=\u001b[39m normalize_image(rdds)\n\u001b[1;32m      5\u001b[0m rdds \u001b[38;5;241m=\u001b[39m denoise_image(rdds)\n\u001b[0;32m----> 6\u001b[0m \u001b[43msave_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 14\u001b[0m, in \u001b[0;36msave_data\u001b[0;34m(image_rdds)\u001b[0m\n\u001b[1;32m     11\u001b[0m     image_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image_data)\n\u001b[1;32m     12\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, image_data)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mimage_rdds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py:1766\u001b[0m, in \u001b[0;36mRDD.foreach\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         f(x)\n\u001b[1;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([])\n\u001b[0;32m-> 1766\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessPartition\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py:2316\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py:2291\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py:2044\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 2044\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n    for x in iterator:\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\ncv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  [Previous line repeated 1 more time]\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n           ^^^^^^^^^^^\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n    for x in iterator:\n  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\ncv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 20:24:10 WARN TaskSetManager: Lost task 70.0 in stage 11.0 (TID 862) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:10 WARN TaskSetManager: Lost task 67.0 in stage 11.0 (TID 859) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 52.0 in stage 11.0 (TID 844) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 56.0 in stage 11.0 (TID 848) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 54.0 in stage 11.0 (TID 846) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 37.0 in stage 11.0 (TID 829) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 69.0 in stage 11.0 (TID 861) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 71.0 in stage 11.0 (TID 863) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 39.0 in stage 11.0 (TID 831) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 58.0 in stage 11.0 (TID 850) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 51.0 in stage 11.0 (TID 843) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 38.0 in stage 11.0 (TID 830) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 30.0 in stage 11 (TID 822) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 0.0 in stage 11 (TID 792) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 29.0 in stage 11 (TID 821) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 31.0 in stage 11 (TID 823) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 28.0 in stage 11 (TID 820) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 792) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 27.0 in stage 11 (TID 819) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 30.0 in stage 11.0 (TID 822) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 25.0 in stage 11 (TID 817) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 22.0 in stage 11 (TID 814) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 23.0 in stage 11 (TID 815) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 21.0 in stage 11 (TID 813) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 29.0 in stage 11.0 (TID 821) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 25.0 in stage 11.0 (TID 817) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 17.0 in stage 11 (TID 809) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 20.0 in stage 11 (TID 812) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 28.0 in stage 11.0 (TID 820) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 26.0 in stage 11 (TID 818) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 24.0 in stage 11 (TID 816) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 31.0 in stage 11.0 (TID 823) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 19.0 in stage 11 (TID 811) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 15.0 in stage 11 (TID 807) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 16.0 in stage 11 (TID 808) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 23.0 in stage 11.0 (TID 815) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 27.0 in stage 11.0 (TID 819) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 18.0 in stage 11 (TID 810) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 7.0 in stage 11 (TID 799) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 11.0 in stage 11 (TID 803) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 10.0 in stage 11 (TID 802) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 14.0 in stage 11 (TID 806) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 8.0 in stage 11 (TID 800) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 9.0 in stage 11 (TID 801) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 22.0 in stage 11.0 (TID 814) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 6.0 in stage 11 (TID 798) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 12.0 in stage 11 (TID 804) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 13.0 in stage 11 (TID 805) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 21.0 in stage 11.0 (TID 813) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:11 WARN PythonRunner: Incomplete task 5.0 in stage 11 (TID 797) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:11 WARN TaskSetManager: Lost task 24.0 in stage 11.0 (TID 816) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 26.0 in stage 11.0 (TID 818) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 16.0 in stage 11.0 (TID 808) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 20.0 in stage 11.0 (TID 812) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 17.0 in stage 11.0 (TID 809) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 15.0 in stage 11.0 (TID 807) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 14.0 in stage 11.0 (TID 806) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 12.0 in stage 11.0 (TID 804) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 19.0 in stage 11.0 (TID 811) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 9.0 in stage 11.0 (TID 801) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 10.0 in stage 11.0 (TID 802) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 8.0 in stage 11.0 (TID 800) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 11.0 in stage 11.0 (TID 803) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 7.0 in stage 11.0 (TID 799) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 6.0 in stage 11.0 (TID 798) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 18.0 in stage 11.0 (TID 810) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 5.0 in stage 11.0 (TID 797) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 13.0 in stage 11.0 (TID 805) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 4.0 in stage 11 (TID 796) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 3.0 in stage 11 (TID 795) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 1.0 in stage 11 (TID 793) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 4.0 in stage 11.0 (TID 796) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 3.0 in stage 11.0 (TID 795) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 1.0 in stage 11.0 (TID 793) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 2.0 in stage 11 (TID 794) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 2.0 in stage 11.0 (TID 794) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 64.0 in stage 11 (TID 856) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 64.0 in stage 11.0 (TID 856) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 62.0 in stage 11 (TID 854) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 36.0 in stage 11.0 (TID 828) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 62.0 in stage 11.0 (TID 854) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 63.0 in stage 11 (TID 855) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 53.0 in stage 11.0 (TID 845) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 63.0 in stage 11.0 (TID 855) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 50.0 in stage 11 (TID 842) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 50.0 in stage 11.0 (TID 842) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 49.0 in stage 11 (TID 841) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 49.0 in stage 11.0 (TID 841) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 48.0 in stage 11 (TID 840) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 48.0 in stage 11.0 (TID 840) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 47.0 in stage 11 (TID 839) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 47.0 in stage 11.0 (TID 839) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 46.0 in stage 11 (TID 838) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 46.0 in stage 11.0 (TID 838) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 45.0 in stage 11 (TID 837) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 45.0 in stage 11.0 (TID 837) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 44.0 in stage 11 (TID 836) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 44.0 in stage 11.0 (TID 836) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 43.0 in stage 11 (TID 835) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 43.0 in stage 11.0 (TID 835) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 42.0 in stage 11 (TID 834) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 42.0 in stage 11.0 (TID 834) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 41.0 in stage 11 (TID 833) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 41.0 in stage 11.0 (TID 833) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 40.0 in stage 11 (TID 832) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 40.0 in stage 11.0 (TID 832) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 35.0 in stage 11 (TID 827) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 35.0 in stage 11.0 (TID 827) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 34.0 in stage 11 (TID 826) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 34.0 in stage 11.0 (TID 826) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 32.0 in stage 11 (TID 824) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 32.0 in stage 11.0 (TID 824) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/12/07 20:24:12 WARN PythonRunner: Incomplete task 33.0 in stage 11 (TID 825) interrupted: Attempting to kill Python Worker\n",
      "24/12/07 20:24:12 WARN TaskSetManager: Lost task 33.0 in stage 11.0 (TID 825) (192.168.1.250 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 66 in stage 11.0 failed 1 times, most recent failure: Lost task 66.0 in stage 11.0 (TID 858) (192.168.1.250 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/rdd.py\", line 1762, in processPartition\n",
      "    for x in iterator:\n",
      "  File \"/home/aphuc/web/data_science/pytorch-notebook/ETL-MI/.venv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2196850/1158716730.py\", line 27, in denoise_image\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/photo/src/denoising.cpp:141: error: (-5:Bad argument) Unsupported depth! Only CV_8U is supported for NORM_L2 in function 'fastNlMeansDenoising'\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "metadata = extract_metadata()\n",
    "rdds = extract_image_data(metadata)\n",
    "rdds = resize_image(rdds)\n",
    "rdds = normalize_image(rdds)\n",
    "rdds = denoise_image(rdds)\n",
    "save_data(rdds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a sample from the saved data\n",
    "loaded_data = np.load(\"output/ct_scans/coronacases_org_001.nii.npy\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "middle_slice = loaded_data.shape[2] // 2\n",
    "plt.imshow(loaded_data[:,:,middle_slice], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_nifti(file_path, password, output_path):\n",
    "    # Read the NIfTI image\n",
    "    nifti_img = nib.load(file_path)\n",
    "    data = nifti_img.get_fdata()\n",
    "    header = nifti_img.header\n",
    "    \n",
    "    # Serialize the data to bytes\n",
    "    data_bytes = data.tobytes()\n",
    "\n",
    "    # Derive a key from the password\n",
    "    salt = os.urandom(16)  # Generate a random salt\n",
    "    kdf = PBKDF2HMAC(\n",
    "        algorithm=hashes.SHA256(),\n",
    "        length=32,\n",
    "        salt=salt,\n",
    "        iterations=100000,\n",
    "        backend=default_backend()\n",
    "    )\n",
    "    key = kdf.derive(password.encode())  # Derive the key\n",
    "\n",
    "    # Encrypt the data\n",
    "    iv = os.urandom(16)  # Initialization vector\n",
    "    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "    encryptor = cipher.encryptor()\n",
    "    \n",
    "    # Padding for block cipher\n",
    "    padder = PKCS7(algorithms.AES.block_size).padder()\n",
    "    padded_data = padder.update(data_bytes) + padder.finalize()\n",
    "\n",
    "    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n",
    "\n",
    "    # Save the encrypted data and metadata\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(salt)  # Write salt for key derivation\n",
    "        f.write(iv)    # Write IV for decryption\n",
    "        f.write(encrypted_data)  # Write encrypted data\n",
    "\n",
    "    print(f\"Encrypted NIfTI image saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted NIfTI image saved to encrypted.nii\n"
     ]
    }
   ],
   "source": [
    "encrypt_nifti(f\"{CT_SCANS}/coronacases_org_001.nii\", \"258456C@p\", \"encrypted.nii\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
